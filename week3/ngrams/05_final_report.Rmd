---
title: "MSD Homework 2, Problem 3"
author: "Your Name (your uni)"
date: '`r Sys.time()`'
output:
  html_document:
    toc: yes
    toc_depth: 3
  pdf_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
library(here)
library(scales)
library(tidyverse)
library(ggplot2)

theme_set(theme_bw())

knitr::opts_chunk$set(echo = TRUE)
```

# Description

This is a template for exercise 6 in Chapter 2 of [Bit By Bit: Social Research in the Digital Age](https://www.bitbybitbook.com/en/1st-ed/observing-behavior/observing-activities/) by Matt Salganik. The problem is reprinted here with some additional comments and structure to facilitate a solution.

The original problem statement:

> In a widely discussed paper, Michel and colleagues ([2011](https://doi.org/10.1126/science.1199644)) analyzed the content of more than five million digitized books in an attempt to identify long-term cultural trends. The data that they used has now been released as the Google NGrams dataset, and so we can use the data to replicate and extend some of their work.
>
> In one of the many results in the paper, Michel and colleagues argued that we are forgetting faster and faster. For a particular year, say “1883,” they calculated the proportion of 1-grams published in each year between 1875 and 1975 that were “1883”. They reasoned that this proportion is a measure of the interest in events that happened in that year. In their figure 3a, they plotted the usage trajectories for three years: 1883, 1910, and 1950. These three years share a common pattern: little use before that year, then a spike, then decay. Next, to quantify the rate of decay for each year, Michel and colleagues calculated the “half-life” of each year for all years between 1875 and 1975. In their figure 3a (inset), they showed that the half-life of each year is decreasing, and they argued that this means that we are forgetting the past faster and faster. They used Version 1 of the English language corpus, but subsequently Google has released a second version of the corpus. Please read all the parts of the question before you begin coding.
>
> This activity will give you practice writing reusable code, interpreting results, and data wrangling (such as working with awkward files and handling missing data). This activity will also help you get up and running with a rich and interesting dataset.

The full paper can be found [here](https://aidenlab.org/papers/Science.Culturomics.pdf), and this is the original figure 3a that you're going to replicate:

> ![](michel_fig_3a.png)

# Part A

> Get the raw data from the [Google Books NGram Viewer website](http://storage.googleapis.com/books/ngrams/books/datasetsv2.html). In particular, you should use version 2 of the English language corpus, which was released on July 1, 2012. Uncompressed, this file is 1.4GB.

## Get and clean the raw data

Edit the `01_download_1grams.sh` file to download the `googlebooks-eng-all-1gram-20120701-1.gz` file and the `02_filter_1grams.sh` file to filter the original 1gram file to only lines where the ngram matches a year (output to a file named `year_counts.tsv`).

Then edit the `03_download_totals.sh` file to down the `googlebooks-eng-all-totalcounts-20120701.txt` and  file and the `04_reformat_totals.sh` file to reformat the total counts file to a valid csv (output to a file named `total_counts.csv`). 

## Load the cleaned data

Load in the `year_counts.tsv` and `total_counts.csv` files. Use the `here()` function around the filename to keep things portable.Give the columns of `year_counts.tsv` the names `term`, `year`, `volume`, and `book_count`. Give the columns of `total_counts.csv` the names `year`, `total_volume`, `page_count`, and `book_count`. Note that column order in these files may not match the examples in the documentation.

```{r load-counts}

total_counts_r <- read_csv('total_counts.csv',
                    col_names = c('year','total_volume','page_count','book_count'))


year_counts_r <- read_tsv('year_counts.tsv', 
              col_names = c('term', 'year', 'volume', 'book_count'))
```

## Your written answer

Add a line below using Rmarkdown's inline syntax to print the total number of lines in each dataframe you've created.

```{r number_lines}

number_lines_in_total_file = nrow(total_counts_r)

number_lines_in_years_file = nrow(year_counts_r)

print(paste("Total number of lines in total counts: " , number_lines_in_total_file))

print(paste("Total number of lines in years file: " , number_lines_in_years_file))


```


# Part B

> Recreate the main part of figure 3a of Michel et al. (2011). To recreate this figure, you will need two files: the one you downloaded in part (a) and the “total counts” file, which you can use to convert the raw counts into proportions. Note that the total counts file has a structure that may make it a bit hard to read in. Does version 2 of the NGram data produce similar results to those presented in Michel et al. (2011), which are based on version 1 data?

## Join ngram year counts and totals

Join the raw year term counts with the total counts and divide to get a proportion of mentions for each term normalized by the total counts for each year.

```{r join-years-and-totals}

head(total_counts_r)

years_and_total <- left_join(year_counts_r, total_counts_r, by = "year")



years_and_total <-  years_and_total |> mutate(
  proportions = volume / total_volume
)

head(years_and_total)


```

## Plot the main figure 3a

Plot the proportion of mentions for the terms "1883", "1910", and "1950" over time from 1850 to 2012, as in the main figure 3a of the original paper. Use the `percent` function from the `scales` package for a readable y axis. Each term should have a different color, it's nice if these match the original paper but not strictly necessary.

```{r plot-proportion-over-time}


years_and_total <- years_and_total |> filter(term == "1883" | term == "1910" | term == "1950") |> filter(year >= "1850" & year <= "2012")

head(years_and_total)

years_and_total |> ggplot(aes(x =  year, y = proportions * 10000, color = term)) + geom_line() +
labs(
  x = "Year", 
  y = "Frequency"
)

```

## Your written answer

Write up your answer to Part B here.

In our graph which is based on version 2, the peaks are a little bit higher than the lines in graph on the Ngram viewer, especially 1950. Also, from what I see is that the graph in our version starts right before the year itself not 10 years compared to the graph in the Ngram version.  


# Part C

> Now check your graph against the graph created by the [NGram Viewer](https://books.google.com/ngrams/).

## Compare to the NGram Viewer

Go to the ngram viewer, enter the terms "1883", "1910", and "1950" and take a screenshot.

`![](ngrams 1883.png)`

Added to what I am menioned in part b, the scales are different on y-axis

## Your written answer

Add your screenshot for Part C below this line using the `![](figure_filename.png)` syntax and comment on similarities / differences.


# Part D

> Recreate figure 3a (main figure), but change the y-axis to be the raw mention count (not the rate of mentions).

## Plot the main figure 3a with raw counts

Plot the raw counts for the terms "1883", "1910", and "1950" over time from 1850 to 2012. Use the `comma` function from the `scales` package for a readable y axis. The colors for each term should match your last plot, and it's nice if these match the original paper but not strictly necessary.

```{r plot-raw-mentions-over-time}

head(years_and_total)

years_and_total |> ggplot(aes(x =  year, y = volume , color = term)) + geom_line() +
scale_y_continuous(labels = comma) +
labs(
  x = "Year", 
  y = "Frequency"
)

```

# Part E

> Does the difference between (b) and (d) lead you to reevaluate any of the results of Michel et al. (2011). Why or why not?

As part of answering this question, make an additional plot.

## Plot the totals

Plot the total counts for each year over time, from 1850 to 2012. Use the `comma` function from the `scales` package for a readable y axis. There should be only one line on this plot (not three).

```{r plot-totals}

view(total_counts_r)

total_counts_r_filtered <- total_counts_r |> filter(year >= "1850" & year <= "2012")



view(total_counts_r_filtered)

total_counts_r_filtered |> ggplot(aes(x =  year, y = total_volume)) + geom_line() +
scale_y_continuous(labels = comma) +
labs(
  x = "Year", 
  y = "Total count"
)

```

## Your written answer

Write up your answer to Part E here.

Yes, this leads to reevaluate any of the results of Michel et al. (2011). I believe now that the people are not forgetting about the year based on this graph. There is a upward trend across the different the three lines for the three years after they reach the peak and go down a little bit. 

# Part F

> Now, using the proportion of mentions, replicate the inset of figure 3a. That is, for each year between 1875 and 1975, calculate the half-life of that year. The half-life is defined to be the number of years that pass before the proportion of mentions reaches half its peak value. Note that Michel et al. (2011) do something more complicated to estimate the half-life—see section III.6 of the Supporting Online Information—but they claim that both approaches produce similar results. Does version 2 of the NGram data produce similar results to those presented in Michel et al. (2011), which are based on version 1 data? (Hint: Don’t be surprised if it doesn’t.)

## Compute peak mentions

For each year term, find the year where its proportion of mentions peaks (hits its highest value). Store this in an intermediate dataframe.

```{r compute-peaks}

all_terms_with_total <- left_join(year_counts_r, total_counts_r, by = "year")

all_terms_with_total <-  all_terms_with_total |> mutate(
  proportions = volume / total_volume
)

str(all_terms_with_total)


all_terms_with_total <-  all_terms_with_total |> filter(grepl("^(18|19|20|21)[0-9]{2}$", term))

view(all_terms_with_total)


#All the terms with peak year and peak value
all_terms_with_peak_year_and_peak_value <- all_terms_with_total |> filter(year >= "1850" & year <= "2012") |> group_by(term) |> 
slice_max(order_by = proportions, n = 1, with_ties = FALSE) |> rename(peak_year = year, peak_proportion = proportions)|> 
select(term, peak_year, peak_proportion)


view(all_terms_with_peak_year_and_peak_value)



# The three terms

all_terms_with_peak_year_and_peak_value |> filter(term %in% c(1883, 1910, 1950))


# It's the numbers but the values are rounded when they are saved in the dataframes

view(years_and_total)
years_and_total_highest_value_for_each_term <- years_and_total |> group_by(term) |> filter(proportions == max(proportions)) |> select(term, year, proportions)
view(years_and_total_highest_value_for_each_term)



```

## Compute half-lifes

Now, for each year term, find the minimum number of years it takes for the proportion of mentions to decline from its peak value to half its peak value. Store this in an intermediate data frame.

```{r compute-half-lifes}


joined_all_terms_with_peak_year_and_peak_value <- all_terms_with_total |> 
inner_join(all_terms_with_peak_year_and_peak_value, by = "term") |> 
filter(year > peak_year) 

view(joined_all_terms_with_peak_year_and_peak_value)


half_life_all_terms <- joined_all_terms_with_peak_year_and_peak_value |> 
filter(proportions <= 0.5 * peak_proportion) |> 
group_by(term) |> 
slice_min(order_by = year, n = 1, with_ties = FALSE) |> 
mutate(years_to_half = year - peak_year, half_year = year) |> 
select(term, half_year, peak_year, years_to_half, peak_proportion, year)


view(half_life_all_terms)

# To view the three main years "terms"

half_life_all_terms |> filter(term %in% c(1883, 1910, 1950))

```

## Plot the inset of figure 3a

Plot the half-life of each term over time from 1850 to 2012. Each point should represent one year term, and add a line to show the trend using `geom_smooth()`.


```{r plot-half-lifes}

half_life_all_terms <- half_life_all_terms |> mutate(highlight = ifelse(term %in% c("1883", "1910", "1950"), term, "Other"))

view(half_life_all_terms)


half_life_all_terms |> filter(term %in% c(1883, 1910, 1950))


half_life_all_terms |> ggplot(aes(x = year, y = years_to_half, color = highlight)) + 
geom_point(size = 4) + 
geom_smooth(method = "loess", se = FALSE, color = "black") + 
scale_x_continuous(limits = c(1850, 2012)) +
scale_color_manual(values = c("1883" =  "blue", "1910" = "#034503", "1950" = "red")) +
labs(
  x = "Year", 
  y = "Half-life (yrs)",
  color = "Terms"
)

```

## Your written answer

Write up your answer to Part F here.

No, they don't look the same as the graph in version 1. In version 1, there is a downward trend in the half-life years over the years. 
The slope in version 1 is clearly negative.  

# Part G

> Were there any years that were outliers such as years that were forgotten particularly quickly or particularly slowly? Briefly speculate about possible reasons for that pattern and explain how you identified the outliers.

Yes, there were some terms that have high half-life, which means that they were hard to be forgotten. 
Also, there were some terms have very short half-life time closer to zero. Below is the code that shows I have identified the outliers

```{r outliers}

view(half_life_all_terms)

str(half_life_all_terms)
#Note about qunatile, it needs a vector to be passed not a dataframe
Q1_half_life <- quantile(half_life_all_terms$years_to_half, 0.25)

Q3_half_life <- quantile(half_life_all_terms$years_to_half, 0.75)

IQR_half_life <- Q3_half_life - Q1_half_life

lower_bound <- Q1_half_life - 1.5* IQR_half_life
upper_bound <- Q3_half_life + 1.5 * IQR_half_life

outliers <- half_life_all_terms |> filter(years_to_half < lower_bound | years_to_half > upper_bound)

print(min(half_life_all_terms$years_to_half))

min_half_life <- min(half_life_all_terms$years_to_half)

terms_min_half_life <- half_life_all_terms |> filter(years_to_half == min_half_life)

max_half_life <- max(half_life_all_terms$years_to_half)

terms_max_half_life <- half_life_all_terms |> filter(years_to_half == max_half_life)


#Outliers 

print("Outliers: ")
print(outliers)
view(outliers)

print("Term with the min years to half life: ")
print(terms_min_half_life)

print("Term with the max years to half life: ")
print(terms_max_half_life)

```



## Your written answer

Write up your answer to Part G here. Include code that shows the years with the smallest and largest half-lifes.

# Makefile

Edit the `Makefile` in this directory to execute the full set of scripts that download the data, clean it, and produce this report. This must be turned in with your assignment such that running `make` on the command line produces the final report as a pdf file.

